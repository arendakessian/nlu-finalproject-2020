{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COXYyx4GyO4n"
   },
   "source": [
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iPp8uBPCtGYZ",
    "outputId": "56a97510-66d8-4500-d26f-e66a7654ca40",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5b406d77f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7Tqg-PQdV-8O",
    "outputId": "813ae807-e88c-4ae6-f505-1511078223bc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "c6NqFAQtttRn",
    "outputId": "9354f71d-5aef-442f-9be2-032d0fcaea00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/Aren/anaconda/lib/python3.5/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9ySXrAjxtwOV",
    "outputId": "c9f55cab-bea5-4b0a-9716-2bc8eda773fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "# The URL for the dataset zip file.\n",
    "url = 'https://competitions.codalab.org/my/datasets/download/4db8bf21-def7-4a86-99f5-7b23d5691bb3'\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "if not os.path.exists('multi-fc/'):\n",
    "    !mkdir multi-fc\n",
    "    wget.download(url, 'multi-fc/multi-fc.zip')\n",
    "    !unzip multi-fc/multi-fc.zip -d multi-fc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "pQ9-afXVt13s",
    "outputId": "129e68b7-6945-4099-e47c-b4d6c231fa22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 27,940\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimID</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>claimURL</th>\n",
       "      <th>reason</th>\n",
       "      <th>categories</th>\n",
       "      <th>speaker</th>\n",
       "      <th>checker</th>\n",
       "      <th>tags</th>\n",
       "      <th>articleTitle</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>claimDate</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>pose-01284</td>\n",
       "      <td>“One of my first acts as president will be to ...</td>\n",
       "      <td>stalled</td>\n",
       "      <td>https://www.politifact.com/truth-o-meter/promi...</td>\n",
       "      <td>None</td>\n",
       "      <td>trumpometer</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Establish a commission on radical Islam</td>\n",
       "      <td>2017-01-17T09:10:41</td>\n",
       "      <td>None</td>\n",
       "      <td>['Islam']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20501</th>\n",
       "      <td>pose-00490</td>\n",
       "      <td>\"Support the principle of network neutrality t...</td>\n",
       "      <td>promise kept</td>\n",
       "      <td>https://www.politifact.com/truth-o-meter/promi...</td>\n",
       "      <td>None</td>\n",
       "      <td>obameter</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Support network neutrality on the Internet</td>\n",
       "      <td>2010-01-07T13:27:01</td>\n",
       "      <td>None</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          claimID                                              claim  \\\n",
       "9879   pose-01284  “One of my first acts as president will be to ...   \n",
       "20501  pose-00490  \"Support the principle of network neutrality t...   \n",
       "\n",
       "              label                                           claimURL reason  \\\n",
       "9879        stalled  https://www.politifact.com/truth-o-meter/promi...   None   \n",
       "20501  promise kept  https://www.politifact.com/truth-o-meter/promi...   None   \n",
       "\n",
       "        categories       speaker checker  tags  \\\n",
       "9879   trumpometer  Donald Trump    None  None   \n",
       "20501     obameter  Barack Obama    None  None   \n",
       "\n",
       "                                     articleTitle          publishDate  \\\n",
       "9879      Establish a commission on radical Islam  2017-01-17T09:10:41   \n",
       "20501  Support network neutrality on the Internet  2010-01-07T13:27:01   \n",
       "\n",
       "      claimDate   entities  \n",
       "9879       None  ['Islam']  \n",
       "20501      None   ['None']  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"multi-fc/train.tsv\", delimiter='\\t', header=None, quoting=csv.QUOTE_NONE, \\\n",
    "                 names= ['claimID', 'claim', 'label', 'claimURL', 'reason', 'categories', 'speaker', \\\n",
    "                  'checker', 'tags', 'articleTitle', 'publishDate', 'claimDate', 'entities'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 2 random rows from the data.\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmyoPfx7t9De"
   },
   "outputs": [],
   "source": [
    "# Contains empty claim\n",
    "indexNames = df[ df['claimID'] == 'bove-00197' ].index\n",
    " \n",
    "# Delete these row indexes from dataFrame\n",
    "df.drop(indexNames , inplace=True)\n",
    "sentences = df.claim.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Evidence Snippets to Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pre_instances[] is a list of claim+snippet pairs\n",
    "\n",
    "claimsnippet_labels[] is a list of the labels (since every claim has one label and is being expanded to claim+label \n",
    "pairs), essentially just an expanded list of the original labels\n",
    "\n",
    "[SEP] token is used not as a BERT separator token, but so that when we tokenize we can properly split up the \n",
    "claim and snippet to pass into encode_plus as separate arguments (see tokenizer section for context)\n",
    "\n",
    "[UNK] token in the exception is used for claims that do not have evidence snippets, BERT can handle this inherently\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_instances = []\n",
    "claimsnippet_labels = []\n",
    "count = 0\n",
    "for a in range(len(list(df.claim))):\n",
    "    claim,claimID,label = list(df.claim)[a], list(df.claimID)[a], list(df.label)[a]\n",
    "    try:\n",
    "        f=open(\"multi-fc/snippets/{claimID}\".format(claimID=claimID), \"r\")\n",
    "        for line in f.readlines():\n",
    "            split = line.split(\"\\t\")\n",
    "            snippet = split[2]\n",
    "            pre_instance =  claim +\" [SEP] \"+snippet\n",
    "            pre_instances.append(pre_instance)\n",
    "            claimsnippet_labels.append(label)\n",
    "    except FileNotFoundError:\n",
    "            pre_instance = claim + \"[SEP]\" + \"[UNK]\"\n",
    "            pre_instances.append(pre_instance)\n",
    "            claimsnippet_labels.append(label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Six out of 10 of the highest unemployment rates are also in so-called right to work states.\" [SEP] May 8, 2013 ... Ron Maag and Kristina Roegner, claiming that \"six out of 10 of the highest  unemployment rates are also in so-called right to work states.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_instances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Encoding Labels and Importing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMjcMXNiQXE6"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "elongated_labels = le.fit_transform(elongated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oe4U3q2DyWkF",
    "outputId": "1eb96d09-b8fc-4dae-8a65-8612efd86771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Viewing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "YaRumNm_yZ4a",
    "outputId": "352138ed-5af9-4b0e-f13f-9ae1f3f22fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  \"Six out of 10 of the highest unemployment rates are also in so-called right to work states.\" [SEP] May 8, 2013 ... Ron Maag and Kristina Roegner, claiming that \"six out of 10 of the highest  unemployment rates are also in so-called right to work states.\n",
      "Tokenized:  ['\"', 'six', 'out', 'of', '10', 'of', 'the', 'highest', 'unemployment', 'rates', 'are', 'also', 'in', 'so', '-', 'called', 'right', 'to', 'work', 'states', '.', '\"', '[SEP]', 'may', '8', ',', '2013', '.', '.', '.', 'ron', 'ma', '##ag', 'and', 'kristina', 'roe', '##gne', '##r', ',', 'claiming', 'that', '\"', 'six', 'out', 'of', '10', 'of', 'the', 'highest', 'unemployment', 'rates', 'are', 'also', 'in', 'so', '-', 'called', 'right', 'to', 'work', 'states', '.']\n",
      "Token IDs:  [1000, 2416, 2041, 1997, 2184, 1997, 1996, 3284, 12163, 6165, 2024, 2036, 1999, 2061, 1011, 2170, 2157, 2000, 2147, 2163, 1012, 1000, 102, 2089, 1022, 1010, 2286, 1012, 1012, 1012, 6902, 5003, 8490, 1998, 28802, 20944, 10177, 2099, 1010, 6815, 2008, 1000, 2416, 2041, 1997, 2184, 1997, 1996, 3284, 12163, 6165, 2024, 2036, 1999, 2061, 1011, 2170, 2157, 2000, 2147, 2163, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', pre_instances[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(pre_instances[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(pre_instances[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our instances are pre_processed, we can pass them into the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "every \"sent\" in pre_sentences is broken up into its constituent claim and snippet using the dummu SEP token\n",
    "      AKA .split(\"SEP\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "fBi3QZ802mEX",
    "outputId": "0700c834-fae5-42ce-ca66-21176bb002ad",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  \"Six out of 10 of the highest unemployment rates are also in so-called right to work states.\" [SEP] May 8, 2013 ... Ron Maag and Kristina Roegner, claiming that \"six out of 10 of the highest  unemployment rates are also in so-called right to work states.\n",
      "Token IDs: tensor([  101,  1000,  2416,  2041,  1997,  2184,  1997,  1996,  3284, 12163,\n",
      "         6165,  2024,  2036,  1999,  2061,  1011,  2170,  2157,  2000,  2147,\n",
      "         2163,  1012,  1000,   102,  2089,  1022,  1010,  2286,  1012,  1012,\n",
      "         1012,  6902,  5003,  8490,  1998, 28802, 20944, 10177,  2099,  1010,\n",
      "         6815,  2008,  1000,  2416,  2041,  1997,  2184,  1997,  1996,  3284,\n",
      "        12163,  6165,  2024,  2036,  1999,  2061,  1011,  2170,  2157,  2000,\n",
      "         2147,  2163,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in pre_instances:\n",
    "    \n",
    "    claim, snippet = sent.split(\"[SEP]\")[0], sent.split(\"[SEP]\")[1]\n",
    "   \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        claim, #claim to encode\n",
    "                        snippet,# snippet to encode\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "elongated_labels = torch.tensor(elongated_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', pre_instances[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "sandbox.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
